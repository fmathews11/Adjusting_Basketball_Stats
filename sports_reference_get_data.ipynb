{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "from modules.constants import MASTER_COLUMN_NAMES\n",
    "SEASON = 2024\n",
    "\n",
    "url_prefix = 'https://www.sports-reference.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get each school's unique URL suffix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an empty dictionary\n",
    "team_name_id_dict = {}\n",
    "\n",
    "# URL for data from all schools\n",
    "all_schools_url = f'https://www.sports-reference.com/cbb/seasons/men/{SEASON}-school-stats.html'\n",
    "response = requests.get(all_schools_url, timeout = 10)\n",
    "soup = BeautifulSoup(response.content)\n",
    "# Find table ('tr')\n",
    "table = soup.findAll('tr')\n",
    "\n",
    "# Iterate through rows\n",
    "for row in table:\n",
    "    # Returns a None object if nothing is found\n",
    "    search = row.find('a',href = True)\n",
    "    # If we have something\n",
    "    if search:\n",
    "\n",
    "        # Extract the name and URL via string manipulation\n",
    "        url_suffix = str(search).split('\"')[1].replace(\".html\",\"\")\n",
    "        team_name = str(search).split(\">\")[1].replace(\"</a\",\"\").strip()\n",
    "        # Update the dictionary\n",
    "        team_name_id_dict[team_name] = url_suffix\n",
    "\n",
    "print(team_name_id_dict['Purdue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate empty data frame\n",
    "master_df = pd.DataFrame()\n",
    "# Create an iteration counter\n",
    "counter = 0\n",
    "# Create a random number between 100 and 300.  This is where the loop will pause\n",
    "# To not overload the site\n",
    "stop_to_rest_point = np.random.randint(20,100)\n",
    "\n",
    "\n",
    "# Iterate through the dictionary\n",
    "for team_name,url_suffix in tqdm(team_name_id_dict.items()):\n",
    "\n",
    "    full_url = f\"{url_prefix}{url_suffix}-gamelogs.html\"\n",
    "    \n",
    "    # If there are NO GAMES, move on\n",
    "    try:\n",
    "        temp_df = pd.read_html(full_url)[0]\n",
    "    except ValueError as e:\n",
    "        msg = str(e)\n",
    "        if msg == \"No tables found\":\n",
    "            continue\n",
    "        raise\n",
    "    \n",
    "    # Surface-level data cleaning\n",
    "    temp_df.columns = [col2 if (col1.startswith('Unnamed') or col1 == \"School\") else f\"opp_{col2}\" for col1,col2 in temp_df.columns]\n",
    "    temp_df = temp_df.iloc[:,~temp_df.columns.str.startswith('Unnamed')].drop('G',axis = 1).dropna().query(\"Date != 'Date'\")\n",
    "    temp_df['team_name'] = team_name\n",
    "    temp_df.columns = MASTER_COLUMN_NAMES\n",
    "    # Appending the cleaned dataframe back to the master data frame\n",
    "    master_df = pd.concat([master_df,temp_df])\n",
    "\n",
    "    # Increment counter\n",
    "    counter +=1\n",
    "    # Sleep and save if we've reached our random number\n",
    "    if counter == stop_to_rest_point:\n",
    "\n",
    "        time.sleep(np.random.randint(60,120))\n",
    "        master_df.to_parquet(f'parquet_files/box_scores_sports_reference_{SEASON}.gzip',compression='gzip')\n",
    "        continue\n",
    "    \n",
    "    # Sleep for 3 to 7 seconds\n",
    "    time.sleep(np.random.randint(3,7))\n",
    "\n",
    "master_df.to_parquet(f'parquet_files/box_scores_sports_reference_{SEASON}.gzip',compression='gzip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual game player box scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Instantiate an empty set to hold game UIs\n",
    "all_game_uis = set()\n",
    "team_game_id_dict = {team:set() for team in team_name_id_dict.keys()}\n",
    "\n",
    "# Create counter and stop to rest point\n",
    "counter = 0\n",
    "stop_to_rest_point = np.random.randint(25,70)\n",
    "\n",
    "for team_name,team_url_section in tqdm(team_name_id_dict.items()):\n",
    "\n",
    "    url = f'{url_prefix}{team_url_section}{\"-gamelogs.html\"}'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Status code was {response.status_code} for {url}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content)\n",
    "    table = soup.findAll(\"tr\")\n",
    "    # Iterate through rows\n",
    "    for row in table:\n",
    "\n",
    "        search = row.find('a',href = True)\n",
    "        # A None object is returned if nothing is found\n",
    "        if not search:\n",
    "            continue\n",
    "            \n",
    "        game_ui = str(search).split('>')[0].replace('<a href=\"/cbb/boxscores/',\"\").replace('.html\"',\"\")\n",
    "\n",
    "        # Games that were forfeited end with the following season, so we want to remove those\n",
    "        if game_ui.endswith(str(SEASON +1)):\n",
    "            continue\n",
    "\n",
    "        all_game_uis.add(game_ui)\n",
    "        team_game_id_dict[team_name].add(game_ui)\n",
    " \n",
    "    counter +=1\n",
    "    if counter == stop_to_rest_point:\n",
    "\n",
    "        time.sleep(np.random.randint(60,120))\n",
    "        continue\n",
    "    \n",
    "    time.sleep(np.random.randint(2,7))\n",
    "\n",
    "# Save the dict as a pickle file\n",
    "with open(f'pickle_files/sports_reference_cbb_teams_and_game_uis_{SEASON}.pickle','wb') as f:\n",
    "    pickle.dump(team_game_id_dict,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(f'pickle_files/game_ids_with_boxscores_{SEASON}.pickle','rb') as f:\n",
    "            game_ui_boxscore_dict = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    game_ui_boxscore_dict = {key:{} for key in set(game_id for v for v in team_game_id_dict.values())}\n",
    "\n",
    "counter = 0\n",
    "save_point = 100\n",
    "\n",
    "# Iterate through the game IDs\n",
    "for game_id, sub_dict in tqdm(game_ui_boxscore_dict.items()):\n",
    "\n",
    "    # Confirmed bad ID\n",
    "    if game_id == \"2022-11-29-20-oral-roberts\":\n",
    "         continue\n",
    "\n",
    "    # Skip if we've already gotten data for this game (list is not empty)\n",
    "    if sub_dict['raw_dataframes']:\n",
    "        continue\n",
    "\n",
    "    url = f\"{url_prefix}/cbb/boxscores/{game_id}.html\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Stop if status code is bad\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Status code returned was {response.status_code} from URL {url}')\n",
    "    \n",
    "\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    first_team = str(soup.find('title')).split('vs.')[0].replace('<title>',\"\").strip()\n",
    "    second_team = str(soup.find('title')).split(\"Box\")[0].split('vs.')[1].strip()\n",
    "    assert first_team,second_team in team_game_id_dict\n",
    "    sub_dict['first_team'] = first_team\n",
    "    sub_dict['second_team'] = second_team\n",
    "\n",
    "    # The last four dataframes of the 20+ returned are what we want\n",
    "    target_dfs = pd.read_html(response.text)[-4:]\n",
    "    sub_dict['raw_dataframes'].extend(target_dfs)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter < save_point:\n",
    "\n",
    "        time.sleep(np.random.randint(3,5))\n",
    "        # Pickle it every so often just in case something happens\n",
    "        with open(f'pickle_files/game_ids_with_boxscores_{SEASON}.pickle','wb') as f:\n",
    "            pickle.dump(game_ui_boxscore_dict,f)\n",
    "        continue\n",
    "\n",
    "    time.sleep(np.random.randint(3,5))\n",
    "\n",
    "# Pickle it\n",
    "with open(f'pickle_files/game_ids_with_boxscores_{SEASON}.pickle','wb') as f:\n",
    "    pickle.dump(game_ui_boxscore_dict,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
